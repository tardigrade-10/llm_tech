{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from env_config import Config\n",
    "openai.api_key = Config.OPENAI_API_KEY\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency Injection is a software design pattern that deals with how components get hold of their dependencies. In other words, it is about how objects supply one another with their dependencies.\n",
      "\n",
      "Here's why it's useful:\n",
      "\n",
      "1. Centralizes Control: The client delegates the responsibility of providing its dependencies to external code (the injector). It centralizes control over the selection and lifecycle of the implementations used in the application.\n",
      "\n",
      "2. Promotes Loose Coupling: Since components do not resolve their dependencies themselves, they can remain unaware of other concrete classes - making the system loosely coupled.\n",
      "\n",
      "3. Enhances Testability: By injecting the dependencies we can provide mock implementations for them, making testing easier.\n",
      "\n",
      "Below is a simple Python example:\n",
      "\n",
      "```python\n",
      "# Consider the case of a class that performs some tasks using a database.\n",
      "# Without Dependency Injection, you might instantiate your database connection directly within the class consuming it.\n",
      "\n",
      "class TaskManagerWithoutDI:\n",
      "    def __init__(self):\n",
      "        self.db = Database() // Connection setup is happening internally\n",
      "\n",
      "    def add_task(self, task):\n",
      "        self.db.add(task)\n",
      "\n",
      "# With Dependency Injection, the connection setup happens externally, \n",
      "# and the instantiated database is \"injected\" into the class that needs it:\n",
      "\n",
      "class TaskManagerWithDI:\n",
      "    def __init__(self, db):\n",
      "        self.db = db  // Database Object is injected\n",
      "\n",
      "    def add_task(self, task):\n",
      "        self.db.add(task)\n",
      "\n",
      "\n",
      "# Now you can create a Database Object externally and inject it.\n",
      "db = Database()\n",
      "Manager = TaskManagerWithDI(db)\n",
      "```\n",
      "\n",
      "Dependency Injection Libraries for Python:\n",
      "Python has several libraries that provide support for implementing Dependency Injection in a Pythonic way. These include `injector`, `dependency_injector`, `serum`, `wiring`, etc. These libraries can provide additional features such as auto-wiring (Automatically inject dependencies without explicitly stating them), scopes (controlling the lifecycle of instances), and more.\n",
      "\n",
      "However, Python's dynamic and interpreted nature often reduces the necessity for such frameworks as shown in the simple example above.\n",
      "\n",
      "In conclusion, dependency injection is a great way to keep your Python code clean, testable, and modular. It enables better control over the dependencies in your application, making your code easier to maintain and refactor."
     ]
    }
   ],
   "source": [
    "\n",
    "system_text = \"You are a brilliant software engineer and your job is to assist the user\"\n",
    "# user_text = '''\n",
    "\n",
    "# '''\n",
    "\n",
    "with open('data/code.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "system_message = {\"role\":\"system\", \"content\": system_text}\n",
    "conversation.append(system_message)\n",
    "\n",
    "# user_message = {\"role\":\"user\", \"content\": text}\n",
    "# conversation.append(user_message)\n",
    "\n",
    "conversation.append({\"role\":\"user\", \"content\": \"\"\"Explain dependency injection for pytho language. provide some examples to explain thoroughly\"\"\"})\n",
    "\n",
    "\n",
    "bot_message = \"\"\n",
    "for token in openai.ChatCompletion.create(\n",
    "                        model = \"gpt-4\",\n",
    "                        messages = conversation,\n",
    "                        stream = True\n",
    "                    ):\n",
    "    t = token[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "    print(t, end=\"\")\n",
    "    bot_message += t\n",
    "\n",
    "bot_message = {\"role\":\"assistant\", \"content\": bot_message}\n",
    "conversation.append(bot_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One way to create a CompletionAgent class that inherits from the BaseAgent and emphasizes on text completion could look something like this:\n",
      "\n",
      "```python\n",
      "class CompletionAgent(BaseAgent):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "    def complete_text(self, text: str):\n",
      "        \"\"\"\n",
      "        The method that will use the given text and try to complete it\n",
      "        using the logic defined in the BaseAgent.\n",
      "        \"\"\"\n",
      "        if not text:\n",
      "            raise ValueError(\"Provided text should not be empty or None\")\n",
      "\n",
      "        # Extract the knowledge base and use it for completion\n",
      "        knowledge = self.knowledge_base\n",
      "\n",
      "        # Here make use of self.prompt_engine or other tools/methods\n",
      "        # to complete the text. This process will depend on the underlying\n",
      "        # implementation of your BaseAgent class.\n",
      "        # For example:\n",
      "        # completed_text = self.prompt_engine.run(text)\n",
      "\n",
      "        # Considering dummy completion here.\n",
      "        completed_text = f\"{text} ...completed\"\n",
      "\n",
      "        return completed_text\n",
      "\n",
      "    # Override the run() method to change the behaviour of the class\n",
      "    def run(self, *args):\n",
      "        # Call the parent run method if needed\n",
      "        # super().run(*args)\n",
      "\n",
      "        # Each argument is considered as text to be completed.\n",
      "        for arg in args:\n",
      "            print(self.complete_text(arg))\n",
      "\n",
      "    def llm_instance(self):\n",
      "        \"\"\"\n",
      "        Override this method to define the logic of creating\n",
      "        a new instance of the language model.\n",
      "        \"\"\"\n",
      "        pass\n",
      "```\n",
      "In this CompletionAgent class, an additional `complete_text(text: str)` method is introduced that takes a text and completes it using the agent's internal logic. The `run()` method is redefined to iterate over the arguments and print the completed text for each one. A `llm_instance()` method is also stubbed in, which can be defined in the subclass to designate how a new instance of the language model is created.\n",
      "\n",
      "Remember that this structure might need to be adjusted and filled according to your actual needs and implementation of the BaseAgent."
     ]
    }
   ],
   "source": [
    "conversation.append({\"role\":\"user\", \"content\": \"\"\"\n",
    "                     Keeping BaseAgent as parent class, create a completion agent class, that takes almost the same input as BaseAgent but instead of generating responses, it completes text.\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self,\n",
    "                knowledge_base: Optional[Any] = None,\n",
    "                tools: Optional[Any] = None,\n",
    "                llm: str = None,\n",
    "                prompt_template: str = None,\n",
    "                input_variables: Dict = {},\n",
    "                agent_id: str = \"default\",\n",
    "                memory: Any = None,\n",
    "                caching: bool = False,\n",
    "                output_key: str = None,\n",
    "                return_complete: bool = False\n",
    "                ):\n",
    "        \n",
    "        self.agent_id = agent_id\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.tools = tools\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template\n",
    "        self.input_variables = input_variables\n",
    "        self.memory = memory\n",
    "        self.state = AgentState.IDLE\n",
    "        self.caching = caching\n",
    "        self.return_complete = return_complete\n",
    "\n",
    "        self.parser = argparse.ArgumentParser(description=\"CLI for interacting with the Agent instance.\")\n",
    "\n",
    "        # print(type(self.knowledge_base))\n",
    "        # print(type(self.knowledge_base.vector_store))\n",
    "        # print(type(self.knowledge_base.data_transformer))\n",
    "        # print()\n",
    "\n",
    "\n",
    "        self.output_key = output_key\n",
    "        self.prompt_engine = prompt_engine(llm = self.llm, template = self.prompt_template, caching=caching)\n",
    "\n",
    "        if self.return_complete and self.output_key is not None:\n",
    "            logging.warning(\"return_complete mode is enabled. Output key will be ignored.\")\n",
    "\n",
    "        if self.knowledge_base is not None and not self.input_variables.get('knowledge_variable'):\n",
    "            raise ValueError (\"knowledge_variable should be present in input_variables while using knowledge\")\n",
    "        \n",
    "        if self.llm is None:\n",
    "            self.llm = self.llm_instance()\"\"\"})\n",
    "\n",
    "bot_message = \"\"\n",
    "for token in openai.ChatCompletion.create(\n",
    "                        model = \"gpt-4\",\n",
    "                        messages = conversation,\n",
    "                        stream = True\n",
    "                    ):\n",
    "    t = token[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "    print(t, end=\"\")\n",
    "    bot_message += t\n",
    "\n",
    "bot_message = {\"role\":\"assistant\", \"content\": bot_message}\n",
    "conversation.append(bot_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/final.txt', 'w') as f:\n",
    "    f.write(conversation[2][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 5, 6: 7, 7: 8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {4:5, 6:7}\n",
    "\n",
    "d[7] = 8\n",
    "d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
